random_num = 0
np.random.seed(random_num)
torch.manual_seed(random_num)
x, y = make_circles(500, noise=0.075)

# Split into train + validation and test sets
x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=0.3, random_state=random_num)

# Further split train + validation into train and validation sets
x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25, random_state=random_num)


# Convert data to PyTorch tensors
x_train_tensor = torch.tensor(x_train_val, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_val, dtype=torch.float32).view(-1, 1)

x_test_tensor = torch.tensor(x_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

# Create TensorDatasets
train_dataset = TensorDataset(x_train_tensor, y_train_tensor)
test_dataset = TensorDataset(x_test_tensor, y_test_tensor)

# data loader
# Training
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Testing
test_loader = DataLoader(test_dataset, batch_size=min(len(test_dataset), 256), shuffle=False)

# Step 1: Define a simple neural network class
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        # Define layers
        self.layer1 = nn.Linear(in_features=2, out_features=1)  
    
    def forward(self, x):
        return torch.sigmoid(self.layer1(x)) 
		
		
# Initialize model, loss function, and optimizer
model = SimpleNN()
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

for epoch in range(epochs):
    model.train()
    epoch_train_loss = 0
    y_train_all = []
    y_train_pred_all = []

    # Training phase
    for x_batch, y_batch in train_loader:
        optimizer.zero_grad()
        y_pred = model(x_batch)  # Predictions already have shape [batch_size, 1]
        loss = criterion(y_pred, y_batch)  # Ensure target matches prediction shape
        loss.backward()
        optimizer.step()

        # Track training loss
        epoch_train_loss += loss.item() * x_batch.size(0)

        # Store true and predicted labels for AUC calculation
        y_train_all.append(y_batch.numpy())
        y_train_pred_all.append(y_pred.detach().numpy().squeeze())  # Squeeze only for AUC calculation

    train_loss.append(epoch_train_loss / len(train_loader.dataset))
    train_auc.append(roc_auc_score(np.concatenate(y_train_all), np.concatenate(y_train_pred_all)))

    # Validation phase
    model.eval()
    epoch_test_loss = 0
    y_test_all = []
    y_test_pred_all = []

    with torch.no_grad():
        for x_batch, y_batch in test_loader:
            y_pred = model(x_batch)  # Predictions already have shape [batch_size, 1]
            loss = criterion(y_pred, y_batch)  # Ensure target matches prediction shape

            # Track validation loss
            epoch_test_loss += loss.item() * x_batch.size(0)

            # Store true and predicted labels for AUC calculation
            y_test_all.append(y_batch.numpy())
            y_test_pred_all.append(y_pred.numpy().squeeze())  # Squeeze only for AUC calculation

    test_loss.append(epoch_test_loss / len(test_loader.dataset))
    test_auc.append(roc_auc_score(np.concatenate(y_test_all), np.concatenate(y_test_pred_all)))

    # Logging every 10 epochs
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch + 1}/{epochs}]")
        print(f"Train Loss: {train_loss[-1]:.4f}, Train AUC: {train_auc[-1]:.4f}")
        print(f"Test Loss: {test_loss[-1]:.4f}, Test AUC: {test_auc[-1]:.4f}")



# Create a 2x2 grid for the subplots
fig, axs = plt.subplots(2, 2, figsize=(12, 10))  # Adjust figsize as needed

# Flatten the axs array to ensure it's a 1D list
axs = axs.flatten()

# Plot Loss per Epoch
axs[0].plot(range(epochs), train_loss, label='Train Loss')
axs[0].plot(range(epochs), test_loss, label='Test Loss')
axs[0].set_title('Loss per Epoch')
axs[0].set_xlabel('Epoch')
axs[0].set_ylabel('Loss')
axs[0].legend()
axs[0].grid()

# Plot AUC per Epoch
axs[1].plot(range(epochs), train_auc, label='Train AUC')
axs[1].plot(range(epochs), test_auc, label='Test AUC')
axs[1].set_title('AUC per Epoch')
axs[1].set_xlabel('Epoch')
axs[1].set_ylabel('AUC')
axs[1].legend()
axs[1].grid()

# Plot ROC Curve
from sklearn.metrics import roc_curve, auc

fpr, tpr, _ = roc_curve(np.concatenate(y_test_all), np.concatenate(y_test_pred_all))
roc_auc = auc(fpr, tpr)
axs[2].plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})")
axs[2].plot([0, 1], [0, 1], 'k--', label="Baseline")
axs[2].set_title('ROC Curve for Test Set')
axs[2].set_xlabel('False Positive Rate')
axs[2].set_ylabel('True Positive Rate')
axs[2].legend()
axs[2].grid()

# Plot Decision Boundary
import numpy as np

xx, yy = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))
grid_points = np.c_[xx.ravel(), yy.ravel()]
with torch.no_grad():
    grid_preds = model(torch.tensor(grid_points, dtype=torch.float32)).numpy().reshape(xx.shape)

axs[3].contourf(xx, yy, grid_preds, levels=50, cmap='RdBu', alpha=0.6)
axs[3].scatter(x_train[:, 0], x_train[:, 1], c=y_train, edgecolor='k', cmap='bwr', label='Train Points')
axs[3].scatter(x_test[:, 0], x_test[:, 1], c=y_test, edgecolor='k', cmap='bwr', marker='x', label='Test Points')
axs[3].set_title('Decision Boundary')
axs[3].set_xlabel('Feature 1')
axs[3].set_ylabel('Feature 2')
axs[3].legend()

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()


