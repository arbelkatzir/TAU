random_num = 0
np.random.seed(random_num)
torch.manual_seed(random_num)
x, y = make_circles(500, noise=0.075)


# Split into train + validation and test sets
x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=0.3, random_state=random_num)

# Further split train + validation into train and validation sets
x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25, random_state=random_num)

# Convert data to PyTorch tensors
x_train_tensor = torch.tensor(x_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Ensure 1D tensor for CrossEntropyLoss

x_val_tensor = torch.tensor(x_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)  # Ensure 1D tensor

x_test_tensor = torch.tensor(x_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)  # Ensure 1D tensor

# Create TensorDatasets
train_dataset = TensorDataset(x_train_tensor, y_train_tensor)
val_dataset = TensorDataset(x_val_tensor, y_val_tensor)
test_dataset = TensorDataset(x_test_tensor, y_test_tensor)

#create batches
batch_size = 32
train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)
val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size) 

class SecondNet(nn.Module):
class SecondNet(nn.Module):
    def __init__(self, in_features, hidden_size, out_features, num_hidden_layers, activation=nn.ReLU(), dropout_prob=0.25):
        super(SecondNet, self).__init__()
        
        self.layers = nn.ModuleList()
        # Input layer
        self.layers.append(nn.Linear(in_features, hidden_size))
        self.layers.append(activation)
        self.layers.append(nn.Dropout(dropout_prob))

        # Hidden layers
        for _ in range(num_hidden_layers):
            self.layers.append(nn.Linear(hidden_size, hidden_size))
            self.layers.append(activation)
            self.layers.append(nn.Dropout(dropout_prob))
        
        # Output layer
        self.output_layer = nn.Linear(hidden_size, out_features)
    
    def forward(self, x):
        # Pass through layers
        for layer in self.layers:
            x = layer(x)  # Ensure each layer is applied to `x`
        return self.output_layer(x)  # Final output logits for CrossEntropyLoss

in_features = 2
out_features = 2  # For binary classification, two logits are required

def objective(trial, num_epochs, X_val, y_val, train_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Set device (use GPU if available, fallback to CPU)
    
    # Set hyperparameters to optimize
    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 3)
    hidden_size = trial.suggest_int('hidden_size', 32, 128)
    dropout_prob = trial.suggest_float('dropout_prob', 0.2, 0.6)
    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)

    # Choose an activation function 
    activation_name = trial.suggest_categorical('activation', ['relu', 'tanh'])
    if activation_name == 'relu':
        activation_fn = nn.ReLU()
    elif activation_name == 'tanh':
        activation_fn = nn.Tanh()

    # Initialize the model with suggested hyperparameters
    model = SecondNet(
        in_features = in_features,
        hidden_size = hidden_size,
        out_features = out_features,
        num_hidden_layers=num_hidden_layers,
        activation=activation_fn,
        dropout_prob=dropout_prob).to(device)

    # Define the loss function and optimizer (these contain HP as well, but they are not part of the actual model)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Initialize params holding best losses so we can save the best trial
    best_accuracy = 0.0
    best_epoch_accuracy = None

    # Train the model - we want to see the results of each *trained* model (model=different set of hps, each trial)
    for epoch in range(num_epochs):
        model.train()
        # Train batches
        for Xt_batch, Yt_batch in train_loader:
            Xt_batch = Xt_batch.to(device)
            Yt_batch = Yt_batch.to(device)  # Already 1D, no `.view(-1, 1)` needed
            optimizer.zero_grad()
            outputs = model(Xt_batch)
            loss = criterion(outputs, Yt_batch)  # Matches logits with integer class labels
            loss.backward()
            optimizer.step()

            
        # Finished all the training batches, evaluate the model on the validation set
        with torch.no_grad():
            model.eval()
            X_val_device = X_val.to(device).float()
            y_val_device = y_val.to(device).long() 
            y_pred = model(X_val_device)
            _, predicted = torch.max(y_pred, 1)
            correct = (predicted == y_val_device).sum().item()
            accuracy = correct / len(y_val)

            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_epoch_accuracy = epoch
                best_model = model.state_dict()  # Save model weights

        # Early stopping check - this is a feature of Optuna's package
        # It can tell by using statistical methods that the samples hps are not going to do well and it can stop the trial before finishing all the epochs
        trial.report(accuracy, step=epoch)
        if trial.should_prune():
            raise optuna.TrialPruned()
            
    # Save best model to trial's user attributes
    trial.set_user_attr("best_model", best_model)

    #print the best validation rmse of this trial and the epoch it was achieved in
    print(f"Best validation accuracy of this trial: {best_accuracy}, epoch num: {best_epoch_accuracy}")
    
    ## another great feature of Optuna is that at the end of each epoch it prints the best trial so far - see it in the progress bar
    return best_accuracy
	
#create a study object and optimize.
num_epochs = 50
study = optuna.create_study(direction='maximize') 

study.optimize(lambda trial: objective(trial, num_epochs, X_val=x_val_tensor, y_val=y_val_tensor, train_loader=train_loader), n_trials=50)


best_trial = study.best_trial
print("Best trial parameters:", best_trial.params)
print("Best validation accuracy:", best_trial.value)


