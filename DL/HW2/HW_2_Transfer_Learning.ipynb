{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 20px; border-radius: 10px; border: 2px solid #3A506B; background-color: #F1F6F9; color: #1C2541;\">\n",
    "    <h1 style=\"font-family: Arial, sans-serif; color: #3A506B; font-size: 2.5em; text-align: center; line-height: 1.2;\">\n",
    "        <span style=\"font-weight: bold;\">Intro to Deep Learning</span>\n",
    "    </h1>\n",
    "    <h2 style=\"font-family: Arial, sans-serif; color: #1C2541; text-align: center; font-size: 1.5em; font-weight: normal;\">\n",
    "        2024-2025\n",
    "    </h2>\n",
    "    <h2 style=\"font-family: Arial, sans-serif; color: #1C2541; text-align: center; font-size: 1.5em; font-weight: normal;\">\n",
    "        Course ID: 05714182\n",
    "    </h2>\n",
    "    <h2 style=\"font-family: Arial, sans-serif; color: #1C2541; text-align: center; font-size: 1.5em; font-weight: normal;\">\n",
    "        ----------------------------------\n",
    "    </h2>\n",
    "    <h2 style=\"font-family: Arial, sans-serif; color: #1C2541; text-align: center; font-size: 1.5em; font-weight: normal;\">\n",
    "        Yahlly Schein\n",
    "    </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T13:24:45.572955Z",
     "iopub.status.busy": "2024-12-04T13:24:45.572549Z",
     "iopub.status.idle": "2024-12-04T13:24:45.586089Z",
     "shell.execute_reply": "2024-12-04T13:24:45.584569Z",
     "shell.execute_reply.started": "2024-12-04T13:24:45.572919Z"
    },
    "tags": []
   },
   "source": [
    "<div style=\"padding: 20px; border-radius: 10px; border: 2px solid #3A506B; background-color: #FF8C00; color: #1C2541;\">\n",
    "    <h2 style=\"font-family: Arial, sans-serif; color: #1C2541; text-align: center; font-size: 1.5em; font-weight: normal;\">\n",
    "        HW 2\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will explore the power of transfer learning by fine-tuning pre-trained deep learning models for image classification. \n",
    "\n",
    "This assignment challenges you to understand how to utilize state-of-the-art models effectively, adapt them to new domains, and make design choices that align with real-world constraints. and yes, this assignment is intentionally more complex than the previous one, as it is designed to encourage you to better understand the material and dive deeper into its underlying concepts.\n",
    "\n",
    "While solving this assingment you will see how training process look in \"real life\". Expect to see some wierd results in the graphs (due to possible mistakes you are going to do) and put some effort into understanding what is going on and fix these issues. This is where you learn. We have talked about everything in class.\n",
    "\n",
    "\n",
    "Read the marked text in this notebook - it will guide you and help you a lot! \n",
    "\n",
    "**By the end, youâ€™ll have practical experience with one of the most impactful techniques in modern deep learning!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 20px; border-radius: 10px; border: 2px solid #3A506B; background-color: #FF3C00; color: #1C2541;\">\n",
    "    <h2 style=\"font-family: Arial, sans-serif; color: #1b1541; text-align: center; font-size: 1.5em; font-weight: normal;\">\n",
    "        ASK YOUR QUESTIONS IN THE DEDICATED Q & A FORUM IN THE MOODLE\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this excercise, you will implement a 2-class classification neural network, using transfer learning from a pre-trained resnet18 network. A pre-trained model is a model that was trained on a large benchmark dataset to solve a problem similar to the one that we want to solve (but not identical). The pre-trained network could have been trained on a different dataset and sometimes also on a different task. The important thing is that it would be a related task and a realted dataset. This way, we can leverage previous learneinng to solve the new problem (instead of training the model from scratch). The ResNet18 model was pre-trained on the [ImageNet](http://image-net.org/index) dataset (a large dataset consisting of 1.4M images and 1000 classes). \n",
    "\n",
    "Transfer learning can be used in two ways:\n",
    "1. Feature extractor: in this way, we remove top layers, freeze the weights of all remaining layers, add a new fully connected layers on top of the pretrained model. Then. we train only these new layers to solve the new task.\n",
    "2. Fine tuning: Here we unfreezing the entire model (or part of it), add a new fully connected layers on top of the pre-trained model (as before) and re-train the entire model on the new data with a very low learning rate. We can use either end-to-end re-training, or re-train only part of the network.\n",
    "\n",
    "### CIFAR-10 Dataset:\n",
    "\n",
    "In this assignment, you will need to perform transfer learning using the ResNet18 network in order to classify and discern between images of cats and dogs in a different dataset: [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1- Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, you need to classify images of 'Cat' and 'Dog' by using transfer learning with the feature extractor approach and a pre-trained ResNet18 network.\n",
    "\n",
    "The pre-trained model is built out of two main parts, the features and the classifier. \n",
    "- The features part is a stack of convolutional layers. You will need to extract the image representation that will be fed into a classifier. \n",
    "- For the classifier, please use a *single fully-connected layer*. \n",
    "\n",
    "\n",
    "In order to make things efficient, start by using the ResNet18 network in order to extract image represetnations for the images in CIFAR-10 dataset. Please make sure to this only once (please do not extract the image representations each time it is needed in the training phase)! You can store the resulting image representations as feature vectors and train the classifier using these vectors. \n",
    "At test time, you can merge both networks in order to achive a single end-to-end classifier.  \n",
    "Note: This is different from what you saw in class.\n",
    "\n",
    "At the test time, you can merge the feature extraction netowork with the new prediction head, in order to recive a \"full solution\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's first import all the packages that you will need during this part of assignment:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to import more packages if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, models, transforms\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ResNet18 network requires images of at least 200x200 pixels. However, the CIFAR-10 dataset consists of small images of 32x32 pixels. Hence, you will need to reshape the images before we use them into the ResNet18 network.\n",
    "\n",
    "Additionally, you will need to match the normalization used while training the ResNet18: each color channel should be normalized separately. \n",
    "\n",
    "The channel means are [0.485, 0.456, 0.406] and the standard deviations are [0.229, 0.224, 0.225]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the transforms library from  torchvision and definethese transformations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify transforms using torchvision.transforms as transforms library\n",
    "from torchvision import transforms\n",
    "\n",
    "# Specify transformations\n",
    "transform = transforms.Compose([\n",
    "    # Resize images to 200x200 pixels\n",
    "    transforms.Resize((200, 200)),\n",
    "    \n",
    "    # Convert images to PyTorch tensors\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize images using ResNet18's pretraining statistics\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # Mean for each channel\n",
    "        std=[0.229, 0.224, 0.225]    # Standard deviation for each channel\n",
    "    )\n",
    "])\n",
    "\n",
    "# Display the transform pipeline\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CIFAR10 dataset and transform dataset from torchvision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset and apply the transformations\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Verify the data shape and transformations\n",
    "print(f\"Train Dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Test Dataset: {len(test_dataset)} samples\")\n",
    "print(f\"Example data shape: {train_dataset[0][0].shape}\")  # Example image tensor shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the CIFAR10 data set includes 10 labels. Please extract from the entire dataset, only the images with 'Cat' or 'Dog' labels.**\n",
    "\n",
    "In order to make the training process faster, you will need to **trim** the train set to 800 training images of each label (800 dogs and 800 cats). \n",
    "The test set should be trimmed to 250 images of cats and 250 images of dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load the full CIFAR-10 dataset\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "full_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Map label names to indices\n",
    "label_map = {\n",
    "    'cat': 3,  # Label index for 'cat'\n",
    "    'dog': 5   # Label index for 'dog'\n",
    "}\n",
    "\n",
    "# Function to extract specific labels\n",
    "def extract_and_trim(dataset, target_labels, num_samples_per_label):\n",
    "    indices = []\n",
    "    counts = {label: 0 for label in target_labels}\n",
    "\n",
    "    # Iterate through the dataset and collect indices of desired labels\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        if label in target_labels and counts[label] < num_samples_per_label:\n",
    "            indices.append(idx)\n",
    "            counts[label] += 1\n",
    "        # Stop when we have enough samples for all labels\n",
    "        if all(count >= num_samples_per_label for count in counts.values()):\n",
    "            break\n",
    "\n",
    "    # Return a subset of the dataset with the selected indices\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# Extract and trim the datasets\n",
    "train_dataset = extract_and_trim(full_train_dataset, [label_map['cat'], label_map['dog']], 800)\n",
    "test_dataset = extract_and_trim(full_test_dataset, [label_map['cat'], label_map['dog']], 250)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Verify the sizes of the datasets\n",
    "print(f\"Train dataset size: {len(train_dataset)} samples\")\n",
    "print(f\"Test dataset size: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the ResNet18 Network from torchvision.models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained ResNet18 model\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Print the model architecture to verify\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, use the pre-trained Resnet18 to extract feautres from the CIFAR10 dataset and feed your classifier with these feature vectors. To do that, you need to cut the head of the pre-trained ResNet18 model, then feed the model with the CIFAR10 data sets (train and test).  The resulting image representations should be \"flattened\" and used as feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet18\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Remove the fully connected (fc) layer to use ResNet18 as a feature extractor\n",
    "feature_extractor = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "feature_extractor.eval()  # Set to evaluation mode\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(data_loader, feature_extractor, device='cpu'):\n",
    "    features = []\n",
    "    labels = []\n",
    "    feature_extractor.to(device)  # Move model to device\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, targets in tqdm(data_loader):\n",
    "            images = images.to(device)\n",
    "            # Pass images through the feature extractor\n",
    "            outputs = feature_extractor(images)\n",
    "            # Flatten the feature maps to vectors\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            features.append(outputs.cpu())\n",
    "            labels.append(targets)\n",
    "    return torch.cat(features), torch.cat(labels)\n",
    "\n",
    "# Device setup (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create DataLoaders for trimmed datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Extract features and labels for training and testing datasets\n",
    "train_features, train_labels = extract_features(train_loader, feature_extractor, device)\n",
    "test_features, test_labels = extract_features(test_loader, feature_extractor, device)\n",
    "\n",
    "# Verify shapes of extracted features\n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with scikit-learn library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, you need to use a logistic regession model from scikit-learn and train it with the data you prepared above. You need to calculate and report the accuracy and AUC for the train and test sets and explain your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with pytorch:\n",
    "\n",
    "#### Next, you need to use a logistic regession using pythourch as follows:\n",
    "\n",
    "\n",
    "1. Explain the choice of architecture in the head of the neural network.\n",
    "2. Plot training loss per epoch.\n",
    "3. Plot AUC per epoch for train and validation sets.\n",
    "4. Explain your results and findings.\n",
    "5. Does adding one or two hidden layers affect the generalization capabilities of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain your choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explain HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explain HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here you should merge the feature extractor network layers with the PyTorch logistic regression classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing  ResNet18 with ResNet34:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the ResNet34 Network from torchvision.models. \n",
    "\n",
    "##### Then, repeat the steps described above in order to compare between these ResNet18 and ResNet43. \n",
    "##### Explain your findings and results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with pytorch for ResNet34:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here you should merge the feature extractor network layers with the PyTorch logistic regression classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the findings and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explain HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2- Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the feature extraction experiment above you were training the classifier on the top of the pre-trained model. \n",
    "\n",
    "In this part, you need to train (or \"fine-tune\") the original pre-trained Resnet18 model (or part of them) in an attempts to improve your results. You can determine the number of convolution layers you intend to re-train in order to avoid overfitting and hopefully achive better generalization. \n",
    "\n",
    "Note that the fine-tuning should only be attempted after you have trained the top-level classifier with a pre-trained model.\n",
    "\n",
    "1. Plot Loss and Accuracy v.s Epoch for train and validation sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model (use the merged feature extractor with the PyTorch logistic regression classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare your results between the two approaches. Is there any improvement? \n",
    "### Explain your findings...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3-Open questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mention 5 concepts/function/blocks of code from the exercises that you used while solving the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mention 3 comcepts/ code parts/ anything that a LLM (ChatGPT, Claude...) helped you with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How practical and meaningfull assingment did you find this one on a scale from 1 (very bad) to 10 (OMG WOW)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Which concepts or techniques became clearer to you while working on this assignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
